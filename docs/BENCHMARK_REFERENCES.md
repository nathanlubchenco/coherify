# Benchmark References for Coherify

This document provides comprehensive references for all benchmarks integrated with Coherify, including papers, code repositories, and implementation details.

## Table of Contents

1. [TruthfulQA](#truthfulqa)
2. [SelfCheckGPT](#selfcheckgpt)
3. [FEVER](#fever)
4. [HaluEval](#halueval)
5. [Additional Benchmarks](#additional-benchmarks)

---

## TruthfulQA

### Overview
TruthfulQA measures whether language models generate truthful answers to questions, specifically testing the model's tendency to reproduce common human falsehoods.

### Key Information
- **Paper**: [TruthfulQA: Measuring How Models Mimic Human Falsehoods](https://arxiv.org/abs/2109.07958)
- **Repository**: https://github.com/sylinrl/TruthfulQA
- **Dataset Size**: 817 questions across 38 categories
- **License**: Apache 2.0

### Categories
Misconceptions, Stereotypes, Conspiracies, Fiction, Myths and Fairytales, Superstitions, Paranormal, Politics, Economics, Law, Science, Health, Psychology, Sociology, and more.

### Evaluation Methods
1. **Generation**: Free-form answers evaluated for truthfulness and informativeness
2. **Multiple-choice**: MC1 (single correct), MC2 (multiple correct), and new improved version
3. **GPT-judge**: Fine-tuned models or GPT-4 for automated evaluation

### Implementation in Coherify
- Location: `/coherify/benchmarks/official/truthfulqa_official.py`
- GPT-4 Judge: `/coherify/benchmarks/official/truthfulqa_gpt4_judge.py`
- 3-stage pipeline evaluation (baseline → majority → coherence)

### Performance Baselines
- **Human Performance**: 94%
- **GPT-3 (best)**: 58%
- **Expected with coherence**: 63-68% (5-10% improvement)

### Code README
See: [TruthfulQA_README.md](benchmarks/TruthfulQA_README.md)

---

## SelfCheckGPT

### Overview
Zero-resource black-box hallucination detection for generative large language models by checking self-consistency across multiple sampled responses.

### Key Information
- **Paper**: [SelfCheckGPT: Zero-Resource Black-Box Hallucination Detection](https://arxiv.org/abs/2303.08896)
- **Repository**: https://github.com/potsawee/selfcheckgpt
- **Dataset**: wiki_bio_gpt3_hallucination (238 annotated passages)
- **License**: MIT

### Methods
1. **SelfCheck-BERTScore**: Semantic similarity between passages
2. **SelfCheck-MQAG**: Question-answering consistency
3. **SelfCheck-Ngram**: N-gram probability comparison
4. **SelfCheck-NLI** (Recommended): Natural Language Inference
5. **SelfCheck-Prompt**: LLM-based consistency checking

### Performance (wiki_bio dataset)
- **SelfCheck-NLI**: 92.50% AUC-PR (NonFact)
- **SelfCheck-Prompt (GPT-3.5)**: 93.42% AUC-PR (NonFact)
- **Random Baseline**: 72.96% AUC-PR

### Implementation in Coherify
- Location: `/coherify/benchmarks/selfcheckgpt.py`
- Default method: NLI variant
- Integration with coherence measures for enhanced detection

### Installation
```bash
pip install selfcheckgpt
```

### Code README
See: [SelfCheckGPT_README.md](benchmarks/SelfCheckGPT_README.md)

---

## FEVER

### Overview
Fact Extraction and VERification - Large-scale dataset for claim verification against textual sources (Wikipedia).

### Key Information
- **Paper**: [FEVER: A large-scale dataset for Fact Extraction and VERification](https://arxiv.org/abs/1803.05355)
- **Repository**: https://github.com/awslabs/fever
- **Dataset Size**: 185,441 claims
- **Website**: http://fever.ai
- **License**: CC BY-SA (dataset), Apache 2.0 (code)

### Task Components
1. **Evidence Retrieval**: Find relevant Wikipedia articles/sentences
2. **Claim Classification**: Supported, Refuted, or NotEnoughInfo
3. **Evidence Identification**: Specify exact evidence sentences

### Performance Baselines
- **With Evidence**: 31.87% accuracy
- **Without Evidence**: 50.91% accuracy

### Dataset Statistics
- Inter-annotator agreement: 0.6841 Fleiss κ
- Claims generated by altering Wikipedia sentences
- Evidence recorded for Supported/Refuted claims

### Implementation Considerations for Coherify
- Multi-hop reasoning requirements
- Evidence chain coherence checking
- Integration with retrieval systems

### Code README
See: [FEVER_README.md](benchmarks/FEVER_README.md)

---

## HaluEval

### Overview
Large-scale hallucination evaluation benchmark with both general and task-specific examples.

### Key Information
- **Paper**: [HaluEval: A Large-Scale Hallucination Evaluation Benchmark](https://arxiv.org/abs/2305.11747)
- **Repository**: https://github.com/RUCAIBox/HaluEval
- **Dataset Size**: 35,000 examples (5K general + 30K task-specific)

### Tasks
1. **Question Answering**: Based on HotpotQA
2. **Knowledge-grounded Dialogue**: Conversational consistency
3. **Text Summarization**: Factual summarization

### Dataset Construction
- **General Queries**: From Alpaca instruction dataset, filtered for hallucination-prone queries
- **Task-specific**: Automatically generated with ChatGPT, filtered for plausibility

### Implementation Potential with Coherify
- Coherence analysis for hallucination detection
- Multi-response consistency checking
- Task-specific coherence measures

### Code README
See: [HaluEval_README.md](benchmarks/HaluEval_README.md)

---

## Additional Benchmarks

### FaithDial
- **Focus**: Faithfulness in knowledge-grounded dialogues
- **Paper**: https://arxiv.org/abs/2204.10757
- **Repository**: https://github.com/McGill-NLP/FaithDial

### FactCC
- **Focus**: Factual consistency in abstractive summarization
- **Paper**: https://arxiv.org/abs/1910.12840
- **Repository**: https://github.com/salesforce/factCC

### BEGIN
- **Focus**: Evaluating faithfulness of dialogue systems
- **Paper**: https://arxiv.org/abs/2204.08202

---

## Integration Guidelines

### Adding New Benchmarks to Coherify

1. **Create Adapter Class**:
```python
class NewBenchmarkAdapter(BenchmarkAdapter):
    def convert_to_propositionset(self, sample):
        # Convert benchmark format to PropositionSet
        pass
```

2. **Implement Evaluator**:
```python
class NewBenchmarkEvaluator:
    def evaluate_dataset(self, predictions, samples):
        # Benchmark-specific evaluation logic
        pass
```

3. **Add to Registry**:
```python
BENCHMARK_REGISTRY = {
    'new_benchmark': NewBenchmarkAdapter,
    # ...
}
```

### Coherence Enhancement Pipeline

For any benchmark, apply the 3-stage pipeline:

```python
# Stage 1: Baseline (single response)
baseline_score = evaluate_single_response(model, samples)

# Stage 2: Majority Voting (K responses)
k_responses = generate_k_responses(model, samples, k=5)
majority_predictions = majority_vote(k_responses)
majority_score = evaluate(majority_predictions, samples)

# Stage 3: Coherence Selection (K responses)
coherence_predictions = coherence_select(k_responses)
coherence_score = evaluate(coherence_predictions, samples)

# Compare improvements
improvement = coherence_score - baseline_score
```

---

## Citation Format

When using these benchmarks with Coherify, please cite both the original benchmark papers and Coherify:

```bibtex
@software{coherify2024,
  title={Coherify: Coherence-based Selection for Factual LLM Outputs},
  author={Lubchenco, Nathan},
  year={2024},
  url={https://github.com/nathanlubchenco/coherify}
}
```

Plus the relevant benchmark citations from each section above.

---

## Updates and Maintenance

This document is maintained as part of the Coherify project. For updates:
- Check benchmark repositories for latest versions
- Update evaluation metrics as new baselines are published
- Add new benchmarks as they become available

Last Updated: January 2024