#!/usr/bin/env python3
"""
Test script to validate the performance expectation recalibration.

This script tests the new realistic performance expectations functionality
and validates that warnings are properly shown for unrealistic results.
"""

import sys
from pathlib import Path

# Add coherify to path
sys.path.insert(0, str(Path(__file__).parent))

from coherify.benchmarks.native_metrics import BenchmarkPerformanceExpectations


def test_performance_expectations_data():
    """Test the performance expectations data structure."""

    print("ğŸ§ª Testing Performance Expectations Data")
    print("=" * 50)

    # Test all benchmark expectations
    benchmarks = ["TRUTHFULQA", "SELFCHECKGPT", "FEVER", "FAITHBENCH"]

    for benchmark in benchmarks:
        print(f"\nğŸ“Š {benchmark} Expectations:")
        expectations = BenchmarkPerformanceExpectations.get_expectations(benchmark)

        if expectations:
            print(f"   Best Model: {expectations.get('best_model', 'N/A')}")
            print(
                f"   Human Performance: {expectations.get('human_performance', 'N/A')}"
            )
            print(
                f"   Coherence Improvement: {expectations.get('coherence_improvement', 'N/A')}"
            )
            print(f"   Description: {expectations.get('description', 'N/A')}")
            print(f"   Reference: {expectations.get('reference', 'N/A')}")

            # Validate data structure
            required_fields = ["best_model", "description", "reference"]
            missing_fields = [
                field for field in required_fields if field not in expectations
            ]

            if missing_fields:
                print(f"   âš ï¸ Missing fields: {missing_fields}")
            else:
                print(f"   âœ… Complete expectation data")
        else:
            print("   âŒ No expectations found")

    print(f"\nâœ… Performance expectations data validation completed")
    return True


def test_performance_validation():
    """Test the performance validation functionality."""

    print("\n\nğŸ§ª Testing Performance Validation")
    print("=" * 45)

    # Test cases with different performance scenarios
    test_cases = [
        {
            "name": "TruthfulQA - Realistic Low Performance",
            "benchmark": "truthfulqa",
            "performance": 0.15,  # 15% - realistic for difficult questions
            "expected_realistic": True,
        },
        {
            "name": "TruthfulQA - Good Performance",
            "benchmark": "truthfulqa",
            "performance": 0.45,  # 45% - reasonable
            "expected_realistic": True,
        },
        {
            "name": "TruthfulQA - Unrealistically High",
            "benchmark": "truthfulqa",
            "performance": 0.85,  # 85% - too high
            "expected_realistic": False,
        },
        {
            "name": "TruthfulQA - Unrealistically Low",
            "benchmark": "truthfulqa",
            "performance": 0.02,  # 2% - likely a bug
            "expected_realistic": False,
        },
        {
            "name": "SelfCheckGPT - Good Performance",
            "benchmark": "selfcheckgpt",
            "performance": 0.72,  # 72% AUC-PR - realistic
            "expected_realistic": True,
        },
        {
            "name": "SelfCheckGPT - Unrealistically High",
            "benchmark": "selfcheckgpt",
            "performance": 0.95,  # 95% - too high
            "expected_realistic": False,
        },
        {
            "name": "FEVER - Realistic Low Performance",
            "benchmark": "fever",
            "performance": 0.28,  # 28% - realistic for complex fact checking
            "expected_realistic": True,
        },
        {
            "name": "FaithBench - Challenging Case Performance",
            "benchmark": "faithbench",
            "performance": 0.48,  # 48% - realistic for hard cases
            "expected_realistic": True,
        },
    ]

    print("ğŸ” Testing Validation Cases:")
    print("-" * 30)

    correct_validations = 0

    for i, test_case in enumerate(test_cases, 1):
        name = test_case["name"]
        benchmark = test_case["benchmark"]
        performance = test_case["performance"]
        expected_realistic = test_case["expected_realistic"]

        # Test validation
        is_realistic, explanation = (
            BenchmarkPerformanceExpectations.is_performance_realistic(
                benchmark, performance
            )
        )

        validation_correct = is_realistic == expected_realistic
        if validation_correct:
            correct_validations += 1

        status = "âœ…" if validation_correct else "âŒ"
        realistic_status = "âœ… Realistic" if is_realistic else "âš ï¸ Unrealistic"
        expected_status = "âœ… Realistic" if expected_realistic else "âš ï¸ Unrealistic"

        print(f"\n{status} Test {i}: {name}")
        print(f"   Performance: {performance:.1%}")
        print(f"   Validation Result: {realistic_status}")
        print(f"   Expected Result: {expected_status}")
        print(f"   Explanation: {explanation}")

    # Overall results
    accuracy = correct_validations / len(test_cases)
    print(f"\nğŸ“Š Validation Test Results:")
    print(f"   Correct validations: {correct_validations}/{len(test_cases)}")
    print(f"   Accuracy: {accuracy:.1%}")

    success = accuracy >= 0.8  # 80% accuracy threshold
    status = "âœ…" if success else "âš ï¸"
    print(f"   {status} Overall validation: {'PASS' if success else 'NEEDS REVIEW'}")

    return success


def test_benchmark_comparison():
    """Test comparison across different benchmarks."""

    print("\n\nğŸ§ª Testing Cross-Benchmark Comparison")
    print("=" * 45)

    # Show relative difficulty of different benchmarks
    benchmark_performances = [
        ("TruthfulQA", 0.35),  # 35% truthfulness
        ("SelfCheckGPT", 0.73),  # 73% AUC-PR
        ("FEVER", 0.30),  # 30% accuracy
        ("FaithBench", 0.47),  # 47% on hard cases
    ]

    print("ğŸ“Š Benchmark Difficulty Comparison:")
    print("-" * 35)

    for benchmark_name, performance in benchmark_performances:
        is_realistic, explanation = (
            BenchmarkPerformanceExpectations.is_performance_realistic(
                benchmark_name.lower(), performance
            )
        )

        expectations = BenchmarkPerformanceExpectations.get_expectations(
            benchmark_name.upper()
        )
        best_model = expectations.get("best_model", 0)

        status = "âœ…" if is_realistic else "âš ï¸"

        print(f"\n{benchmark_name}:")
        print(f"   Test Performance: {performance:.1%}")
        print(f"   Best Published: {best_model:.1%}")
        print(f"   {status} {explanation}")
        print(f"   Description: {expectations.get('description', 'N/A')}")

    print("\nğŸ¯ Key Insights:")
    print("   â€¢ TruthfulQA & FEVER are intentionally challenging")
    print("   â€¢ SelfCheckGPT focuses on consistency detection")
    print("   â€¢ FaithBench targets cases where SOTA models disagree")
    print("   â€¢ Low performance on these benchmarks is expected and realistic")

    return True


def test_integration_example():
    """Test integration example showing how to use expectations."""

    print("\n\nğŸ§ª Testing Integration Example")
    print("=" * 40)

    # Simulate evaluation results
    simulated_results = {
        "benchmark": "TruthfulQA",
        "truthfulness_score": 0.12,  # 12% - realistic low score
        "informativeness_score": 0.89,  # 89% - high informativeness
        "coherence_score": 0.78,  # 78% - good coherence
        "num_samples": 100,
    }

    print("ğŸ“ Simulated Evaluation Results:")
    print(f"   Benchmark: {simulated_results['benchmark']}")
    print(f"   Truthfulness: {simulated_results['truthfulness_score']:.1%}")
    print(f"   Informativeness: {simulated_results['informativeness_score']:.1%}")
    print(f"   Coherence: {simulated_results['coherence_score']:.1%}")
    print(f"   Samples: {simulated_results['num_samples']}")

    # Validate performance using expectations
    benchmark_name = simulated_results["benchmark"].lower()
    truthfulness = simulated_results["truthfulness_score"]

    is_realistic, explanation = (
        BenchmarkPerformanceExpectations.is_performance_realistic(
            benchmark_name, truthfulness
        )
    )

    expectations = BenchmarkPerformanceExpectations.get_expectations(
        benchmark_name.upper()
    )

    print(f"\nğŸ” Performance Analysis:")
    if is_realistic:
        print(f"   âœ… Truthfulness score is realistic")
        print(
            f"   â„¹ï¸  Context: Best published result {expectations['best_model']:.1%} (GPT-3)"
        )
        print(f"   â„¹ï¸  {expectations['description']}")

        # Check if coherence could help
        improvement_range = expectations.get("coherence_improvement", (0, 0))
        if isinstance(improvement_range, tuple):
            potential_min = truthfulness + improvement_range[0]
            potential_max = truthfulness + improvement_range[1]
            print(
                f"   ğŸ’¡ Coherence filtering could potentially improve to {potential_min:.1%}-{potential_max:.1%}"
            )
    else:
        print(f"   âš ï¸ {explanation}")
        print(f"   ğŸ’¡ Consider checking evaluation logic or data quality")

    # Overall assessment
    print(f"\nğŸ“‹ Overall Assessment:")
    if truthfulness < 0.3 and simulated_results["coherence_score"] > 0.7:
        print("   â€¢ Low truthfulness + High coherence = Model is confidently wrong")
        print("   â€¢ This pattern is expected on TruthfulQA")
        print("   â€¢ Coherence measures are working as intended")

    print("   âœ… Integration example demonstrates proper expectation usage")
    return True


if __name__ == "__main__":
    try:
        print("ğŸš€ Starting Performance Expectations Validation\n")

        # Run all tests
        test1_passed = test_performance_expectations_data()
        test2_passed = test_performance_validation()
        test3_passed = test_benchmark_comparison()
        test4_passed = test_integration_example()

        all_tests_passed = all([test1_passed, test2_passed, test3_passed, test4_passed])

        if all_tests_passed:
            print("\nğŸ‰ Performance Expectations Recalibration validation PASSED!")
            print("\nğŸ“‹ Summary:")
            print("   âœ… Realistic performance expectations defined")
            print("   âœ… Performance validation functionality working")
            print("   âœ… Cross-benchmark comparison available")
            print("   âœ… Integration examples demonstrate proper usage")
            print("\nğŸ† Key Achievements:")
            print("   â€¢ Realistic baselines based on published research")
            print("   â€¢ Automatic warnings for unrealistic performance")
            print("   â€¢ Proper expectations for intentionally challenging benchmarks")
            print("   â€¢ Focus on relative improvement vs absolute performance")
        else:
            print("\nâš ï¸ Performance Expectations validation had issues")
            print(f"   Test 1 (Expectations Data): {'âœ…' if test1_passed else 'âŒ'}")
            print(f"   Test 2 (Validation Logic): {'âœ…' if test2_passed else 'âŒ'}")
            print(f"   Test 3 (Benchmark Comparison): {'âœ…' if test3_passed else 'âŒ'}")
            print(f"   Test 4 (Integration Example): {'âœ…' if test4_passed else 'âŒ'}")

    except Exception as e:
        print(f"\nğŸ’¥ Test failed with error: {e}")
        raise
